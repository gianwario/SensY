# Online Appendix â€” Bias Ahead: Sensitive Prompts as Early Warnings for Fairness in Large Language Models

This repository contains the complete online appendix for the study
**â€œPrompt Sensitiveness in Large Language Modelsâ€**, including datasets, scripts, experimental configurations, and reproducibility material for RQ1 and RQ2.

The repository is organized into three main components:

1. **Dataset construction** â€” all sources used to build the SENSY dataset.
2. **RQ1 evaluation** â€” scripts and raw LLM outputs used to assess adequacy on sensitive prompts.
3. **RQ2 classification pipeline** â€” code, data, and trained models for automated sensitivity prediction.

---

## ğŸ“ Repository Structure
```text
SENSY/
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ prompts_chatbot_arena.json     # Human-generated prompts annotated as sensitive/non-sensitive
â”‚   â”œâ”€â”€ prompts_chatgpt.json           # Synthetically generated prompts (ChatGPT)
â”‚   â””â”€â”€ SENSY.json                     # Final merged & labeled dataset used in RQ2
â”‚   â””â”€â”€ dataset_analysis.py                     # Python script to give details about the dataset SENSY
â”‚
â”œâ”€â”€ RQ1/
â”‚   â”œâ”€â”€ sample.json                    # Sample of 500 sensitive prompts used in RQ1
â”‚   â”œâ”€â”€ llama_response.json            # Raw responses from LLaMA (3 runs per prompt)
â”‚   â”œâ”€â”€ qwen_response.json             # Raw responses from Qwen (3 runs per prompt)
â”‚   â”œâ”€â”€ deepseek_response.json         # Raw responses from DeepSeek (3 runs per prompt)
â”‚   â””â”€â”€ rq1_llm_query.py               # Script used to query local LLMs through LM Studio API
â”‚
â”œâ”€â”€ RQ2/
â”‚   â”œâ”€â”€ data/                          # Training/test sets automatically derived during experiments
â”‚   â”œâ”€â”€ models/                        # Saved Random Forest models (optional depending on size)
â”‚   â”œâ”€â”€ preprocessing/                 # Tokenization, feature extraction, and cleaning utilities
â”‚   â”œâ”€â”€ samples/                       # Example predictions and error analysis logs
â”‚   â”œâ”€â”€ common_functions.py            # Shared utilities (loading, metrics, plotting)
â”‚   â”œâ”€â”€ extract_single.py              # Prompt features extraction
â”‚   â”œâ”€â”€ predict_sensitive.py           # Module to test the trained classifier
â”‚   â””â”€â”€ main.py                        # Full training and evaluation pipeline for the SENSY classifier
â”‚
â””â”€â”€ README.md
```
---

## Datasets

### 1. Synthetic prompts (`prompts_chatgpt.json`)

Generated using ChatGPT following the sensitivity definition adopted in the study.Each prompt is annotated as:

- 1 â€” sensitive
- 0 â€” non-sensitive

### 2. Chatbot Arena prompts (`prompts_chatbot_arena.json`)

Sampled from the LMSYS *Chatbot Arena Conversations* dataset.
Only first-turn user prompts were retained.
All items were manually annotated using a coding-by-consensus process.

### 3. Final unified dataset (`SENSY.json`)

Used in **RQ2** to train and test the SensY classifier.Contains:

- the sensitivity label
- the domain category
- preprocessing metadata

---

## RQ1 â€” Evaluating LLM Adequacy on Sensitive Prompts

This study evaluates whether sensitive prompts elicit inadequate or problematic responses from local LLMs.

### Included Files

- `sample.json` â€” Balanced set of 500 sensitive prompts used for evaluation.
- `{model}_response.json` â€” The 4,500 total responses (3 models Ã— 500 prompts Ã— 3 runs).
- `rq1_llm_query.py` â€” Script querying local models via LM Studio REST API.

---

---

## RQ2 â€” Automatic Prediction of Prompt Sensitiveness

This folder contains the full pipeline for the *SensY* classifier, including preprocessing, feature extraction, model training, and error analysis.

### Running the classifier

Train the model:

```bash
python main.py
```

Use the model:

```bash
python predict_sensitive.py
```

---
